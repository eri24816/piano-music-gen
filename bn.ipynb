{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Iterable, List, Mapping, Sequence\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Factor:\n",
    "    def __init__(self, table: torch.Tensor|list|float, variables: List[str]):\n",
    "        self.variables = variables\n",
    "        if not isinstance(table, torch.Tensor):\n",
    "            table = torch.tensor(table)\n",
    "        self.table = table\n",
    "        assert len(table.shape) == len(variables) # the last dimension is the value. The rest are variables\n",
    "\n",
    "    def __mul__(self, other: 'Factor') -> 'Factor':\n",
    "        '''\n",
    "        Multiply two factors.\n",
    "        '''\n",
    "        var_union = set(self.variables) | set(other.variables)\n",
    "\n",
    "        var_name_map = {var: chr(ord('a') + i) for i, var in enumerate(var_union)}\n",
    "\n",
    "        var_output = self.variables + [var for var in other.variables if var not in self.variables]\n",
    "\n",
    "        def to_einsum_string(str_list: Iterable[str]) -> str:\n",
    "            return ''.join(map(lambda x: var_name_map[x], str_list))\n",
    "        result = torch.einsum(\n",
    "            to_einsum_string(self.variables)+','+to_einsum_string(other.variables) \n",
    "            + '->' \n",
    "            + to_einsum_string(var_output), self.table, other.table)\n",
    "        \n",
    "        return Factor(result, var_output)\n",
    "    \n",
    "    def __truediv__(self, other: 'Factor') -> 'Factor':\n",
    "        return self * other.inverse()\n",
    "    \n",
    "    def __getitem__(self, index: Mapping[str, int|None|slice]) -> torch.Tensor:\n",
    "        arr_index:Sequence[slice|int] = [slice(None)] * len(self.variables)\n",
    "        for var, val in index.items():\n",
    "            if val is None:\n",
    "                continue\n",
    "            if isinstance(val, slice):\n",
    "                arr_index[self.variables.index(var)] = val\n",
    "            else:\n",
    "                arr_index[self.variables.index(var)] = slice(val, val+1)\n",
    "        return self.table[tuple(arr_index)]\n",
    "    \n",
    "    def marginalize(self, var_name: str| list[str]) -> 'Factor':\n",
    "        '''\n",
    "        Marginalize the factor with respect to the variable.\n",
    "        '''\n",
    "        if isinstance(var_name, str):\n",
    "            var_name = [var_name]\n",
    "        result = torch.sum(self.table, dim= tuple([self.variables.index(var) for var in var_name]))\n",
    "        var_output = [var for var in self.variables if var not in var_name]\n",
    "        return Factor(result, var_output)\n",
    "    \n",
    "    def inverse(self) -> 'Factor':\n",
    "        '''\n",
    "        Inverse the factor.\n",
    "        '''\n",
    "        return Factor(1/self.table, self.variables)\n",
    "\n",
    "class Variable(nn.Module):\n",
    "    def __init__(self, name:str, cpt: torch.Tensor|List, parents: List['Variable'] = []):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.parents = parents\n",
    "        if isinstance(cpt, list):\n",
    "            cpt = torch.tensor(cpt)\n",
    "        self.cpt = Factor(cpt, [parent.name for parent in parents] + [name])\n",
    "\n",
    "class BayesianNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.variables: nn.ModuleDict = nn.ModuleDict()\n",
    "\n",
    "    def copy(self):\n",
    "        new_instance = BayesianNetwork()\n",
    "        for var in self.variables:\n",
    "            assert isinstance(var, Variable)\n",
    "            new_instance.add_variable(Variable(var.name,var.cpt.table.clone(),var.parents))\n",
    "\n",
    "    def add_variable(self, variable: Variable):\n",
    "        self.variables[variable.name] = variable\n",
    "\n",
    "    def get_variable(self, name: str) -> Variable:\n",
    "        return self.variables[name] # type: ignore\n",
    "    \n",
    "    def infer(self, target: Dict[str,int|None|slice], observation: Dict[str, torch.Tensor] = {}) -> Factor:\n",
    "        '''\n",
    "        p(target|observation)\n",
    "        '''\n",
    "        joint = self.get_joint_distribution(observation)\n",
    "\n",
    "        # marginalize dont care variables\n",
    "        dont_cares = [var for var in joint.variables if var not in target]\n",
    "        joint = joint.marginalize(dont_cares)\n",
    "\n",
    "        target_var = [var for var in joint.variables if var in target]\n",
    "\n",
    "        return Factor(joint[target], target_var)\n",
    "\n",
    "    def get_joint_distribution(self, observation: Dict[str, torch.Tensor] = {}) -> Factor:\n",
    "        '''\n",
    "        Get the joint distribution of all variables.\n",
    "        '''\n",
    "        joint = Factor(1, [])\n",
    "        for var in self.variables.values():\n",
    "            if var.name in observation:\n",
    "                joint = joint * Factor(observation[var.name], [var.name])\n",
    "            else:\n",
    "                joint = joint * var.cpt\n",
    "        return joint\n",
    "    \n",
    "    def get_entropy(self, target: str|list[str], observation: Dict[str, torch.Tensor] = {}, condition: str|List[str]=[]) -> float:\n",
    "        '''\n",
    "        Get the entropy of target variable given the condition.\n",
    "        '''\n",
    "        if isinstance(target, str):\n",
    "            target = [target]\n",
    "\n",
    "        if isinstance(condition, str):\n",
    "            condition = [condition]\n",
    "\n",
    "        dist = self.infer({var: None for var in target + condition}, observation)\n",
    "        cond_dist = dist.marginalize(target)\n",
    "        log_term = (dist / cond_dist).table.log2()\n",
    "        log_term[log_term.isinf() | log_term.isnan()] = 0\n",
    "        return -(dist.table * log_term).sum().item()\n",
    "    \n",
    "    def sample(self, n: int, target: List[str]|None = None, observation: Dict[str, torch.Tensor] = {}) -> Dict[str, torch.Tensor]:\n",
    "        '''\n",
    "        Sample from the Bayesian Network.\n",
    "        '''\n",
    "        joint = self.get_joint_distribution(observation)\n",
    "        sample = torch.multinomial(joint.table.view(-1), n, replacement=True)\n",
    "        result = {}\n",
    "        for i, var in enumerate(reversed(joint.variables)):\n",
    "            if target is None or var in target:\n",
    "                result[var] = sample % joint.table.shape[i]\n",
    "            sample = sample // joint.table.shape[i]\n",
    "        return {var: result[var].view(-1) for var in result}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bn = BayesianNetwork()\n",
    "\n",
    "# # Define the variables\n",
    "# a = Variable('a', [.96,.04])\n",
    "# b = Variable('b', [[.5,.5,0],[0,0,1]], [a])\n",
    "# c = Variable('c', [[.5,.5,0],[0,0,1]], [a])\n",
    "# x0 = Variable('x0', [[1,0],[0,1],[0,1]], [b])\n",
    "# x1 = Variable('x1', [[1,0],[0,1],[1,0]], [b])\n",
    "# x2 = Variable('x2', [[1,0],[0,1],[1,0]], [c])\n",
    "# x3 = Variable('x3', [[1,0],[0,1],[0,1]], [c])\n",
    "\n",
    "# # Add the variables to the network\n",
    "# for var in [a,b,c,x0,x1,x2,x3]:\n",
    "#     bn.add_variable(var)\n",
    "\n",
    "# # Infer the distribution of x3 given a=0\n",
    "# bn.infer({'x0':None,'x1':None}).table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_model = BayesianNetwork()\n",
    "a = Variable('a', [.25,.25,.25,.25])\n",
    "b = Variable('b', [[.7,.1,.1,.1],[.1,.7,.1,.1],[.1,.1,.7,.1],[.1,.1,.1,.7]], [a])\n",
    "c = Variable('c', [[.7,.1,.1,.1],[.1,.7,.1,.1],[.1,.1,.7,.1],[.1,.1,.1,.7]], [a])\n",
    "x0 = Variable('x0', [[.94, .02, .02, .02],[.02, .94, .02, .02],[.02, .02, .94, .02],[.02, .02, .02, .94]], [b])\n",
    "x1 = Variable('x1', [[.94, .02, .02, .02],[.02, .94, .02, .02],[.02, .02, .94, .02],[.02, .02, .02, .94]], [b])\n",
    "x2 = Variable('x2', [[.94, .02, .02, .02],[.02, .94, .02, .02],[.02, .02, .94, .02],[.02, .02, .02, .94]], [c])\n",
    "x3 = Variable('x3', [[.94, .02, .02, .02],[.02, .94, .02, .02],[.02, .02, .94, .02],[.02, .02, .02, .94]], [c])\n",
    "\n",
    "for var in [a,b,c,x0,x1,x2,x3]:\n",
    "    real_model.add_variable(var)\n",
    "\n",
    "dataset = real_model.sample(100)\n",
    "dataset_tensor = torch.stack([dataset[var] for var in ['x0', 'x1', 'x2', 'x3']], dim=1) # [100, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from itertools import product\n",
    "\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, dataset: torch.Tensor):\n",
    "        pass\n",
    "\n",
    "    def get_likelihood(self, data) -> torch.Tensor:\n",
    "        '''\n",
    "        Returns the likelihood of the data.\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "    \n",
    "def kld(p: torch.Tensor, q: torch.Tensor) -> float:\n",
    "    '''\n",
    "    Compute the KL divergence between two distributions.\n",
    "    '''\n",
    "    assert p.shape == q.shape\n",
    "    items = p * (p / q).log()\n",
    "    items[p == 0] = 0\n",
    "    return items.sum().item()\n",
    "\n",
    "def eval_model(model: Model, real_dist: Factor) -> float:\n",
    "    '''\n",
    "    Evaluate the model with the dataset.\n",
    "    '''\n",
    "    likelihood = torch.zeros([4,4,4,4])\n",
    "    for x in product(range(4), repeat=4):\n",
    "        likelihood[x] = model.get_likelihood(torch.tensor(x)).item()\n",
    "\n",
    "    real = real_dist.marginalize(['a','b','c']).table\n",
    "\n",
    "    return kld(real, likelihood)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: frequency of each possible value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FreqModel(Model):\n",
    "    def __init__(self):\n",
    "        self.freq = {}\n",
    "\n",
    "    def fit(self, dataset: torch.Tensor, smoothing_prior: float = 0):\n",
    "        data, count = torch.unique(dataset, return_counts=True, dim=0)\n",
    "\n",
    "        all_possibilities = torch.tensor([[i,j,k,l] for i in range(4) for j in range(4) for k in range(4) for l in range(4)])\n",
    "\n",
    "        for i, possibility in enumerate(all_possibilities):\n",
    "            self.freq[tuple(possibility.tolist())] = smoothing_prior * len(data) / len(all_possibilities)\n",
    "\n",
    "        for i, possibility in enumerate(data):\n",
    "            self.freq[tuple(possibility.tolist())] += count[i].item()\n",
    "\n",
    "        total = sum(self.freq.values())\n",
    "        for key in self.freq.keys():\n",
    "            self.freq[key] /= total\n",
    "\n",
    "    def get_likelihood(self, data) -> torch.Tensor:\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(0)\n",
    "        return torch.tensor([self.freq[tuple(data[i].tolist())] for i in range(data.shape[0])]).prod()\n",
    "    \n",
    "    def get_log_likelihood(self, data) -> torch.Tensor:\n",
    "        if len(data.shape) == 1:\n",
    "            data = data.unsqueeze(0)\n",
    "        return torch.tensor([self.freq[tuple(data[i].tolist())] for i in range(data.shape[0])]).log().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FreqModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m freq_model \u001b[38;5;241m=\u001b[39m \u001b[43mFreqModel\u001b[49m()\n\u001b[0;32m      2\u001b[0m freq_model\u001b[38;5;241m.\u001b[39mfit(dataset_tensor, smoothing_prior\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m eval_model(freq_model, real_model\u001b[38;5;241m.\u001b[39mget_joint_distribution())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FreqModel' is not defined"
     ]
    }
   ],
   "source": [
    "freq_model = FreqModel()\n",
    "freq_model.fit(dataset_tensor, smoothing_prior=1)\n",
    "\n",
    "eval_model(freq_model, real_model.get_joint_distribution())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999949"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(freq_model.freq.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.8800000000000002e-05"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.6*0.3*0.02*0.01*0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
